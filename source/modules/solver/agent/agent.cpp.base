#include "auxiliar/fs.hpp"
#include "engine.hpp"
#include "modules/solver/agent/agent.hpp"
#include "sample/sample.hpp"
#include <chrono>
#include <omp.h>

__startNamespace__;

// Declare reduction clause for vectors
#pragma omp declare reduction(vec_int_plus : std::vector<int> : std::transform(omp_out.begin(), omp_out.end(), omp_in.begin(), omp_out.begin(), std::plus <int>())) initializer(omp_priv = omp_orig)

void __className__::initialize()
{
  // Get variable count
  _variableCount = _k->_variables.size();
  
  // Getting problem pointer
  _problem = dynamic_cast<problem::ReinforcementLearning *>(_k->_problem);

  // Check compatibility of settings
  if( (_multiAgentSampling == "Experiences" || _multiPolicyUpdate == "Together") && _problem->_policiesPerEnvironment != 1 )
    KORALI_LOG_ERROR("Sampling experiences is not compatible with multiple policies.\n");

  // Formatting reward history for each agent
  _trainingRewardHistory.resize(_problem->_agentsPerEnvironment);
  
  // Allocating and obtaining action bounds information
  _actionLowerBounds.resize(_problem->_actionVectorSize);
  _actionUpperBounds.resize(_problem->_actionVectorSize);

  for (size_t i = 0; i < _problem->_actionVectorSize; i++)
  {
    auto varIdx = _problem->_actionVectorIndexes[i];
    _actionLowerBounds[i] = _k->_variables[varIdx]->_lowerBound;
    _actionUpperBounds[i] = _k->_variables[varIdx]->_upperBound;

    if (_actionUpperBounds[i] - _actionLowerBounds[i] <= 0.0) KORALI_LOG_ERROR("Upper (%f) and Lower Bound (%f) of action variable %lu invalid.\n", _actionUpperBounds[i], _actionLowerBounds[i], i);
  }

  // Initializing selected policy
  initializeAgent();

  // Initializing random seed for the shuffle operation
  mt = new std::mt19937(rd());
  mt->seed(_k->_randomSeed++);

  // If not set, using heurisitc for maximum size
  if (_experienceReplayMaximumSize == 0)
    _experienceReplayMaximumSize = std::pow(2, 14) * std::sqrt(_problem->_stateVectorSize + _problem->_actionVectorSize);

  // If not set, filling ER before learning
  if (_experienceReplayStartSize == 0)
    _experienceReplayStartSize = _experienceReplayMaximumSize;

  // Initialize current beta for all agents
  _experienceReplayOffPolicyREFERCurrentBeta = std::vector<float>(_problem->_agentsPerEnvironment, _experienceReplayOffPolicyREFERBeta);

  //  Pre-allocating space for the experience replay memory
  _stateVector.resize(_experienceReplayMaximumSize);
  _actionVector.resize(_experienceReplayMaximumSize);
  _retraceValueVector.resize(_experienceReplayMaximumSize);
  _rewardVector.resize(_experienceReplayMaximumSize);
  _stateValueVector.resize(_experienceReplayMaximumSize);
  _importanceWeightVector.resize(_experienceReplayMaximumSize);
  _truncatedImportanceWeightVector.resize(_experienceReplayMaximumSize);
  _truncatedStateValueVector.resize(_experienceReplayMaximumSize);
  _truncatedStateVector.resize(_experienceReplayMaximumSize);
  _terminationVector.resize(_experienceReplayMaximumSize);
  _expPolicyVector.resize(_experienceReplayMaximumSize);
  _curPolicyVector.resize(_experienceReplayMaximumSize);
  _isOnPolicyVector.resize(_experienceReplayMaximumSize);
  _episodePosVector.resize(_experienceReplayMaximumSize);
  _episodeIdVector.resize(_experienceReplayMaximumSize);

  //  Pre-allocating space for state time sequence
  _stateTimeSequence.resize(_timeSequenceLength);

  /*********************************************************************
   * If initial generation, set initial agent configuration
   *********************************************************************/
  if (_k->_currentGeneration == 0)
  {
    _currentEpisode = 0;
    _policyUpdateCount = 0;
    _currentSampleID = 0;
    _experienceCount = 0;

    // Initializing training and episode statistics //TODO go through all
    _testingAverageReward = -korali::Inf;
    _testingBestReward = -korali::Inf;
    _testingWorstReward = -korali::Inf;
    _testingBestAverageReward = -korali::Inf;
    _testingBestEpisodeId = 0;
    _trainingBestReward.resize(_problem->_agentsPerEnvironment, -korali::Inf);
    _trainingBestEpisodeId.resize(_problem->_agentsPerEnvironment, 0);
    _trainingAverageReward.resize(_problem->_agentsPerEnvironment, -korali::Inf);

    /* Initializing REFER information */

    // If cutoff scale is not defined, use a heuristic value [defaults to 4.0]
    if (_experienceReplayOffPolicyCutoffScale < 0.0f)
      KORALI_LOG_ERROR("Experience Replay Cutoff Scale must be larger 0.0");
    _experienceReplayOffPolicyCount.resize(_problem->_agentsPerEnvironment, 0);
    _experienceReplayOffPolicyRatio.resize(_problem->_agentsPerEnvironment, 0.0f);
    _currentLearningRate = _learningRate;

    _experienceReplayOffPolicyCurrentCutoff = _experienceReplayOffPolicyCutoffScale;

    // Rescaling information
    _stateRescalingMeans = std::vector<std::vector<float>>(_problem->_agentsPerEnvironment, std::vector<float>(_problem->_stateVectorSize, 0.0f));
    _stateRescalingSigmas = std::vector<std::vector<float>>(_problem->_agentsPerEnvironment, std::vector<float>(_problem->_stateVectorSize, 1.0f));
    _rewardRescalingSigma = std::vector<float>(_problem->_agentsPerEnvironment, 1.0f);
    _rewardRescalingSumSquaredRewards = std::vector<float>(_problem->_agentsPerEnvironment, 0.0f);

    // Getting agent's initial policy
    _trainingCurrentPolicies = getAgentPolicy();
  }

  // Setting current agent's training state
  setAgentPolicy(_trainingCurrentPolicies["Policy Hyperparameters"]);

  // If this continues a previous training run, deserialize previous input experience replay. Only for the root (engine) rank
  if (_k->_currentGeneration > 0)
    if (_mode == "Training" || _trainingBestPolicies.empty())
     if (_k->_engine->_conduit != NULL)
      deserializeExperienceReplay();

  // Initializing session-wise profiling timers
  _sessionRunningTime = 0.0;
  _sessionSerializationTime = 0.0;
  _sessionAgentComputationTime = 0.0;
  _sessionAgentCommunicationTime = 0.0;
  _sessionAgentPolicyEvaluationTime = 0.0;
  _sessionPolicyUpdateTime = 0.0;
  _sessionAgentAttendingTime = 0.0;

  _generationMiniBatchGenerationTime = 0.0;
  _generationStateSequenceTime = 0.0;
  _generationRunPolicyTime = 0.0;
  _generationUpdateExperienceMetadataTime = 0.0;
  _generationCalculatePolicyGradientsTime = 0.0;
  _generationRunGenerationTime = 0.0;

  // Initializing session-specific counters
  _sessionExperienceCount = 0;
  _sessionEpisodeCount = 0;
  _sessionGeneration = 1;
  _sessionPolicyUpdateCount = 0;

  // Calculating how many more experiences do we need in this session to reach the starting size
  _sessionExperiencesUntilStartSize = _stateVector.size() > _experienceReplayStartSize ? 0 : _experienceReplayStartSize - _stateVector.size();

  if (_mode == "Training")
  {
    // Creating storate for _agents and their status
    _agents.resize(_concurrentEnvironments);
    _isAgentRunning.resize(_concurrentEnvironments, false);
  }

  if (_mode == "Testing")
  {
    // Fixing termination criteria for testing mode
    _maxGenerations = _k->_currentGeneration + 1;

    // Setting testing policy to best testing hyperparameters if not custom-set by the user
    if (_testingCurrentPolicies.empty())
    {
      // Checking if testing policies have been generated
      if (_testingBestPolicies.empty())
      {
        _k->_logger->logWarning("Minimal", "Trying to test policy, but no testing policies have been generated during training yet or given in the configuration. Using current training policy instead.\n");
        _testingCurrentPolicies = _trainingCurrentPolicies;
      }
      else
      {
        _testingCurrentPolicies = _testingBestPolicies;
      }
    }

    // Checking if there's testing samples defined
    if (_testingSampleIds.size() == 0)
      KORALI_LOG_ERROR("For testing, you need to indicate the sample ids to run in the ['Testing']['Sample Ids'] field.\n");

    // Prepare storage for rewards from tested samples
    _testingReward.resize(_testingSampleIds.size());
  }
}

void __className__::runGeneration()
{
  if (_mode == "Training") trainingGeneration();
  if (_mode == "Testing") testingGeneration();
}

void __className__::trainingGeneration()
{
  auto beginTime = std::chrono::steady_clock::now(); // Profiling

  // Setting generation-specific timers
  _generationRunningTime = 0.0;
  _generationSerializationTime = 0.0;
  _generationAgentComputationTime = 0.0;
  _generationAgentCommunicationTime = 0.0;
  _generationAgentPolicyEvaluationTime = 0.0;
  _generationPolicyUpdateTime = 0.0;
  _generationAgentAttendingTime = 0.0;

  _generationMiniBatchGenerationTime = 0.0;
  _generationStateSequenceTime = 0.0;
  _generationRunPolicyTime = 0.0;
  _generationUpdateExperienceMetadataTime = 0.0;
  _generationCalculatePolicyGradientsTime = 0.0;
  _generationRunGenerationTime = 0.0;

  // Running until all _agents have finished
  while (_sessionEpisodeCount < _episodesPerGeneration * _sessionGeneration)
  {
    // Launching (or re-launching) agents
    for (size_t agentId = 0; agentId < _concurrentEnvironments; agentId++)
      if (_isAgentRunning[agentId] == false)
      {
        _agents[agentId]["Sample Id"] = _currentSampleID++;
        _agents[agentId]["Module"] = "Problem";
        _agents[agentId]["Operation"] = "Run Training Episode";
        for (size_t p = 0; p < _problem->_policiesPerEnvironment; p++)
          _agents[agentId]["Policy Hyperparameters"][p] = _trainingCurrentPolicies["Policy Hyperparameters"][p];
        _agents[agentId]["State Rescaling"]["Means"] = _stateRescalingMeans;
        _agents[agentId]["State Rescaling"]["Standard Deviations"] = _stateRescalingSigmas;

        KORALI_START(_agents[agentId]);

        _isAgentRunning[agentId] = true;
      }

    // Listening to _agents for incoming experiences
    KORALI_LISTEN(_agents);

    // Attending to running agents, checking if any experience has been received
    for (size_t agentId = 0; agentId < _concurrentEnvironments; agentId++)
      if (_isAgentRunning[agentId] == true)
        attendAgent(agentId);

    // Perform optimization steps on the critic/policy, if reached the minimum replay memory size
    if (_experienceCount >= _experienceReplayStartSize)
    {
      // If we accumulated enough experiences, we rescale the states (once)
      if (_stateRescalingEnabled == true)
        if (_policyUpdateCount == 0)
          rescaleStates();

      // If we accumulated enough experiences between updates in this session, update now
      while (_sessionExperienceCount > (_experiencesBetweenPolicyUpdates * _sessionPolicyUpdateCount + _sessionExperiencesUntilStartSize))
      {
        // Calling the algorithm specific policy training algorithm
        auto beginTime = std::chrono::steady_clock::now(); // Profiling
        trainPolicy();
        auto endTime = std::chrono::steady_clock::now(); // Profiling
        _sessionPolicyUpdateTime += std::chrono::duration_cast<std::chrono::nanoseconds>(endTime - beginTime).count(); // Profiling
        _generationPolicyUpdateTime += std::chrono::duration_cast<std::chrono::nanoseconds>(endTime - beginTime).count(); // Profiling

        // Increasing policy update counters
        _policyUpdateCount++;
        _sessionPolicyUpdateCount++;

        // Updating REFER off-policy cutoff
        _experienceReplayOffPolicyCurrentCutoff = _experienceReplayOffPolicyCutoffScale / (1.0f + _experienceReplayOffPolicyAnnealingRate * (float)_policyUpdateCount);

        for (size_t d = 0; d < _problem->_agentsPerEnvironment; d++)
        {
          // Updating REFER learning rate and beta parameters
          _currentLearningRate = _learningRate / (1.0f + _experienceReplayOffPolicyAnnealingRate * (float)_policyUpdateCount);
          if (_experienceReplayOffPolicyRatio[d] > _experienceReplayOffPolicyTarget)
            _experienceReplayOffPolicyREFERCurrentBeta[d] = (1.0f - _currentLearningRate) * _experienceReplayOffPolicyREFERCurrentBeta[d];
          else
            _experienceReplayOffPolicyREFERCurrentBeta[d] = (1.0f - _currentLearningRate) * _experienceReplayOffPolicyREFERCurrentBeta[d] + _currentLearningRate;
        }
      }

      // Getting new policy hyperparameters (for agents to generate actions)
      _trainingCurrentPolicies = getAgentPolicy();
    }
  }

  // Now serializing experience replay database
  if (_experienceReplaySerialize == true)
    if (_k->_fileOutputEnabled)
      if (_k->_fileOutputFrequency > 0)
        if (_k->_currentGeneration % _k->_fileOutputFrequency == 0)
          serializeExperienceReplay();

  // Measuring generation time
  auto endTime = std::chrono::steady_clock::now();                                                             // Profiling
  _sessionRunningTime += std::chrono::duration_cast<std::chrono::nanoseconds>(endTime - beginTime).count();    // Profiling
  _generationRunningTime += std::chrono::duration_cast<std::chrono::nanoseconds>(endTime - beginTime).count(); // Profiling

  /*********************************************************************
   * Updating statistics/bookkeeping
   *********************************************************************/

  // Updating average cumulative reward statistics
  _trainingAverageReward = std::vector<float>(_problem->_agentsPerEnvironment, 0.0f);
  for (size_t d = 0; d < _problem->_agentsPerEnvironment; d++)
  {
    ssize_t startEpisodeId = _trainingRewardHistory[d].size() - _trainingAverageDepth;
    ssize_t endEpisodeId = _trainingRewardHistory[d].size() - 1;
    if (startEpisodeId < 0) startEpisodeId = 0;
    for (ssize_t e = startEpisodeId; e <= endEpisodeId; e++)
      _trainingAverageReward[d] += _trainingRewardHistory[d][e];
    _trainingAverageReward[d] /= (float)(endEpisodeId - startEpisodeId + 1);
  }

  // Increasing session's generation count
  _sessionGeneration++;
}

void __className__::testingGeneration()
{
  // Allocating testing agents
  std::vector<Sample> testingAgents(_testingSampleIds.size());

  // Launching  agents
  for (size_t sampleId = 0; sampleId < _testingSampleIds.size(); sampleId++)
  {
    testingAgents[sampleId]["Sample Id"] = _testingSampleIds[sampleId];
    testingAgents[sampleId]["Module"] = "Problem";
    testingAgents[sampleId]["Operation"] = "Run Testing Episode";
    for (size_t p = 0; p < _problem->_policiesPerEnvironment; p++)
      testingAgents[sampleId]["Policy Hyperparameters"][p] = _testingCurrentPolicies["Policy Hyperparameters"][p];
    testingAgents[sampleId]["State Rescaling"]["Means"] = _stateRescalingMeans;
    testingAgents[sampleId]["State Rescaling"]["Standard Deviations"] = _stateRescalingSigmas;

    KORALI_START(testingAgents[sampleId]);
  }

  KORALI_WAITALL(testingAgents);

  for (size_t sampleId = 0; sampleId < _testingSampleIds.size(); sampleId++)
    _testingReward[sampleId] = testingAgents[sampleId]["Testing Reward"].get<float>();
}

void __className__::rescaleStates()
{
  // Calculation of state moments
  std::vector<std::vector<float>> sumStates(_problem->_agentsPerEnvironment, std::vector<float>(_problem->_stateVectorSize, 0.0f));
  std::vector<std::vector<float>> squaredSumStates(_problem->_agentsPerEnvironment, std::vector<float>(_problem->_stateVectorSize, 0.0f));

  for (size_t i = 0; i < _stateVector.size(); ++i)
    for (size_t j = 0; j < _problem->_agentsPerEnvironment; ++j)
      for (size_t d = 0; d < _problem->_stateVectorSize; ++d)
      {
        sumStates[j][d] += _stateVector[i][j][d];
        squaredSumStates[j][d] += _stateVector[i][j][d] * _stateVector[i][j][d];
      }

  for (size_t j = 0; j < _problem->_agentsPerEnvironment; ++j)
  {
    _k->_logger->logInfo("Detailed", " + Agent %ld using State Normalization N(Mean, Sigma):\n", j);
    for (size_t d = 0; d < _problem->_stateVectorSize; ++d)
    {
      _stateRescalingMeans[j][d] = sumStates[j][d] / (float)_stateVector.size();
      if (std::isfinite(_stateRescalingMeans[j][d]) == false) _stateRescalingMeans[j][d] = 0.0f;

      _stateRescalingSigmas[j][d] = std::sqrt(squaredSumStates[j][d] / (float)_stateVector.size() - _stateRescalingMeans[j][d] * _stateRescalingMeans[j][d]);
      if (std::isfinite(_stateRescalingSigmas[j][d]) == false) _stateRescalingSigmas[j][d] = 1.0f;
      if (_stateRescalingSigmas[j][d] <= 1e-9) _stateRescalingSigmas[j][d] = 1.0f;

      _k->_logger->logInfo("Detailed", " + State [%zu]: N(%f, %f)\n", d, _stateRescalingMeans[j][d],  _stateRescalingSigmas[j][d]);
    }
  }

  // Actual rescaling of initial states
  for (size_t i = 0; i < _stateVector.size(); ++i)
    for (size_t j = 0; j < _problem->_agentsPerEnvironment; ++j)
      for (size_t d = 0; d < _problem->_stateVectorSize; ++d)
        _stateVector[i][j][d] = (_stateVector[i][j][d] - _stateRescalingMeans[j][d]) / _stateRescalingSigmas[j][d];
}

void __className__::attendAgent(size_t agentId)
{
  auto beginTime = std::chrono::steady_clock::now(); // Profiling

  // Storage for the incoming message
  knlohmann::json message;

  // Retrieving the experience, if any has arrived for the current agent.
  if (_agents[agentId].retrievePendingMessage(message))
  {
    // If agent requested new policy, send the new hyperparameters
    if (message["Action"] == "Request New Policy")
    {
      KORALI_SEND_MSG_TO_SAMPLE(_agents[agentId], _trainingCurrentPolicies);
    }

    // Process episode(s) incoming from the agent(s)
    if (message["Action"] == "Send Episodes")
    {
      // Process every episode received and its experiences (add them to replay memory)
      processEpisode(message["Episodes"]);

      // Waiting for the agent to come back with all the information
      KORALI_WAIT(_agents[agentId]);

      // Getting the training reward of the latest episodes
      _trainingLastReward = _agents[agentId]["Training Rewards"].get<std::vector<float>>();

      // Keeping training statistics. Updating if exceeded best training policy so far.
      for (size_t d = 0; d < _problem->_agentsPerEnvironment; d++)
      {
        if (_trainingLastReward[d] > _trainingBestReward[d])
        {
          _trainingBestReward[d] = _trainingLastReward[d];
          _trainingBestEpisodeId[d] = _currentEpisode;
          if (_problem->_policiesPerEnvironment == 1)
            _testingBestPolicies["Policy Hyperparameters"][0] = _agents[agentId]["Policy Hyperparameters"][0];
          else
            _testingBestPolicies["Policy Hyperparameters"][d] = _agents[agentId]["Policy Hyperparameters"][d];
        }
        _trainingRewardHistory[d].push_back(_trainingLastReward[d]);
      }
      // Storing bookkeeping information
      _trainingExperienceHistory.push_back(message["Episodes"]["Experiences"].size());

      // If the policy has exceeded the threshold during training, we gather its statistics
      if (_agents[agentId]["Tested Policy"] == true)
      {
        _testingCandidateCount++;
        _testingBestReward = _agents[agentId]["Best Testing Reward"].get<float>();
        _testingWorstReward = _agents[agentId]["Worst Testing Reward"].get<float>();
        _testingAverageReward = _agents[agentId]["Average Testing Reward"].get<float>();
        _testingAverageRewardHistory.push_back(_testingAverageReward);

        // If the average testing reward is better than the previous best, replace it
        // and store hyperparameters as best so far.
        if (_testingAverageReward > _testingBestAverageReward)
        {
          _testingBestAverageReward = _testingAverageReward;
          _testingBestEpisodeId = _currentEpisode;
          for (size_t d = 0; d < _problem->_policiesPerEnvironment; ++d)
            _testingBestPolicies["Policy Hyperparameters"][d] = _agents[agentId]["Policy Hyperparameters"][d];
        }
      }

      // Obtaining profiling information
      _sessionAgentComputationTime += _agents[agentId]["Computation Time"].get<double>();
      _sessionAgentCommunicationTime += _agents[agentId]["Communication Time"].get<double>();
      _sessionAgentPolicyEvaluationTime += _agents[agentId]["Policy Evaluation Time"].get<double>();
      _generationAgentComputationTime += _agents[agentId]["Computation Time"].get<double>();
      _generationAgentCommunicationTime += _agents[agentId]["Communication Time"].get<double>();
      _generationAgentPolicyEvaluationTime += _agents[agentId]["Policy Evaluation Time"].get<double>();

      // Set agent as finished
      _isAgentRunning[agentId] = false;
    }
  }

  auto endTime = std::chrono::steady_clock::now();                                                                    // Profiling
  _sessionAgentAttendingTime += std::chrono::duration_cast<std::chrono::nanoseconds>(endTime - beginTime).count();    // Profiling
  _generationAgentAttendingTime += std::chrono::duration_cast<std::chrono::nanoseconds>(endTime - beginTime).count(); // Profiling
}

void __className__::processEpisode(knlohmann::json &episode)
{
  /*********************************************************************
   * Adding episode's experiences into the replay memory
   *********************************************************************/

  // Getting this episode's Id from the global counter
  size_t episodeId = _currentEpisode;

  // Getting experience count from the episode
  size_t curExperienceCount = episode["Experiences"].size();

  // Storage for the episode's cumulative reward
  std::vector<float> cumulativeReward(_problem->_agentsPerEnvironment, 0.0f);

  // Go over experiences in episode
  const size_t episodeExperienceCount = episode["Experiences"].size();
  for (size_t expId = 0; expId < episodeExperienceCount; expId++)
  {
    // Put state to replay memory
    _stateVector.add(episode["Experiences"][expId]["State"].get<std::vector<std::vector<float>>>());

    // Get action and put it to replay memory
    auto action = episode["Experiences"][expId]["Action"].get<std::vector<std::vector<float>>>();
    _actionVector.add(action);

    // Get reward
    std::vector<float> reward = episode["Experiences"][expId]["Reward"].get<std::vector<float>>();

    // For cooporative multi-agent model rewards are averaged
    if (_multiAgentRelationship == "Cooperation")
    {
      float avgReward = std::accumulate(reward.begin(), reward.end(), 0.);
      avgReward /= _problem->_agentsPerEnvironment;
      reward = std::vector<float>(_problem->_agentsPerEnvironment, avgReward);
    }

    // Update reward rescaling moments
    if (_rewardRescalingEnabled)
    {
      if (_rewardVector.size() >= _experienceReplayMaximumSize)
      {
        for (size_t d = 0; d < _problem->_agentsPerEnvironment; d++)
          _rewardRescalingSumSquaredRewards[d] -= _rewardVector[0][d] * _rewardVector[0][d];
      }
      for (size_t d = 0; d < _problem->_agentsPerEnvironment; d++)
      {
        _rewardRescalingSumSquaredRewards[d] += reward[d] * reward[d];
      }
    }

    // Put reward to replay memory
    _rewardVector.add(reward);

    // Keeping statistics
    for (size_t d = 0; d < _problem->_agentsPerEnvironment; d++)
      cumulativeReward[d] += reward[d];

    // Checking and adding experience termination status, truncated state and truncated state value to replay memory
    termination_t termination;
    std::vector<std::vector<float>> truncatedState;

    if (episode["Experiences"][expId]["Termination"] == "Non Terminal") termination = e_nonTerminal;
    if (episode["Experiences"][expId]["Termination"] == "Terminal") termination = e_terminal;
    if (episode["Experiences"][expId]["Termination"] == "Truncated")
    {
      termination = e_truncated;
      truncatedState = episode["Experiences"][expId]["Truncated State"].get<std::vector<std::vector<float>>>();
    }

    _terminationVector.add(termination);
    _truncatedStateVector.add(truncatedState);

    // Getting policy information and state value
    std::vector<policy_t> expPolicy(_problem->_agentsPerEnvironment);
    std::vector<float> stateValue(_problem->_agentsPerEnvironment);

    if (isDefined(episode["Experiences"][expId], "Policy", "State Value"))
    {
      stateValue = episode["Experiences"][expId]["Policy"]["State Value"].get<std::vector<float>>();
      for (size_t d = 0; d < _problem->_agentsPerEnvironment; d++)
      {
        expPolicy[d].stateValue = stateValue[d];
      }
    }
    else
    {
      KORALI_LOG_ERROR("Policy has not produced state value for the current experience.\n");
    }
    _stateValueVector.add(stateValue);

    // Adding placeholder for retrace value (will be updated below)
    _retraceValueVector.add(std::vector<float>(_problem->_agentsPerEnvironment,0.0f));

    /* Story policy information for continuous action spaces */
    if (isDefined(episode["Experiences"][expId], "Policy", "Distribution Parameters"))
    {
      const auto distParams = episode["Experiences"][expId]["Policy"]["Distribution Parameters"].get<std::vector<std::vector<float>>>();
      for (size_t d = 0; d < _problem->_agentsPerEnvironment; d++)
        expPolicy[d].distributionParameters = distParams[d];
    }

    if (isDefined(episode["Experiences"][expId], "Policy", "Unbounded Action"))
    {
      const auto unboundedAc = episode["Experiences"][expId]["Policy"]["Unbounded Action"].get<std::vector<std::vector<float>>>();
      for (size_t d = 0; d < _problem->_agentsPerEnvironment; d++)
        expPolicy[d].unboundedAction = unboundedAc[d];
    }

    /* Story policy information for discrete action spaces */
    if (isDefined(episode["Experiences"][expId], "Policy", "Action Index"))
    {
      const auto actIdx = episode["Experiences"][expId]["Policy"]["Action Index"].get<std::vector<size_t>>();
      for (size_t d = 0; d < _problem->_agentsPerEnvironment; d++)
        expPolicy[d].actionIndex = actIdx[d];
    }

    if (isDefined(episode["Experiences"][expId], "Policy", "Action Probabilities"))
    {

      const auto actProb = episode["Experiences"][expId]["Policy"]["Action Probabilities"].get<std::vector<std::vector<float>>>();
      for (size_t d = 0; d < _problem->_agentsPerEnvironment; d++)
        expPolicy[d].actionProbabilities = actProb[d];
    }

    if (isDefined(episode["Experiences"][expId], "Policy", "Available Actions"))
    {
      const auto availAct = episode["Experiences"][expId]["Policy"]["Available Actions"].get<std::vector<std::vector<bool>>>();
      for (size_t d = 0; d < _problem->_agentsPerEnvironment; d++)
      {
        expPolicy[d].availableActions = availAct[d];
        if (expPolicy[d].availableActions.size() > 0)
          if (std::accumulate(expPolicy[d].availableActions.begin(), expPolicy[d].availableActions.end(), 0) == 0)
            KORALI_LOG_ERROR("State with experience id %zu for agent %zu detected with no available actions.", expId, d);
      }
    }

    // Storing policy information in replay memory
    _expPolicyVector.add(expPolicy);
    _curPolicyVector.add(expPolicy);

    // Storing Episode information in replay memory
    _episodeIdVector.add(episodeId);
    _episodePosVector.add(expId);

    // If outgoing experience is off policy, subtract off policy counter
    if (_isOnPolicyVector.size() == _experienceReplayMaximumSize)
    for (size_t d = 0; d < _problem->_agentsPerEnvironment; d++)
    if (_isOnPolicyVector[0][d] == false)
    {
      _experienceReplayOffPolicyCount[d]--;
    }

    // Adding new experience's on policiness (by default is true when adding it to the ER)
    _isOnPolicyVector.add(std::vector<bool>(_problem->_agentsPerEnvironment, true));

    // Initialize experience's importance weight (1.0 because its freshly produced)
    _importanceWeightVector.add(std::vector<float>(_problem->_agentsPerEnvironment, 1.0f));
    _truncatedImportanceWeightVector.add(std::vector<float>(_problem->_agentsPerEnvironment, 1.0f));
  }

  /*********************************************************************
   * Computing initial retrace value for the newly added experiences
   *********************************************************************/

  // Getting position of the final experience of the episode in the replay memory
  ssize_t endId = (ssize_t)_stateVector.size() - 1;

  // Getting the starting ID of the initial experience of the episode in the replay memory
  ssize_t startId = endId - episodeExperienceCount + 1;

  // Storage for the retrace value
  std::vector<float> retV(_problem->_agentsPerEnvironment, 0.0f);

  // If it was a truncated episode, add the value function for the terminal state to retV
  if (_terminationVector[endId] == e_truncated)
  {
    for (size_t d = 0; d < _problem->_agentsPerEnvironment; d++)
    {
      // Get truncated state
      auto expTruncatedStateSequence = getTruncatedStateSequence(endId, d);

      // Forward Policy for truncated state
      std::vector<policy_t> truncatedPolicyInfo;
      if (_problem->_policiesPerEnvironment == 1)
        runPolicy( {expTruncatedStateSequence}, truncatedPolicyInfo );
      else
        runPolicy( {expTruncatedStateSequence}, truncatedPolicyInfo, d );

      // Update retV with truncated value
      retV[d] = truncatedPolicyInfo[0].stateValue;

      // Get value of trucated state
      if (std::isfinite(retV[d]) == false)
        KORALI_LOG_ERROR("Calculated state value for truncated state returned an invalid value: %f\n", retV[d]);
    }

    // For cooporative multi-agent model truncated state-values are averaged
    if (_multiAgentRelationship == "Cooperation")
    {
      float avgRetV = std::accumulate(retV.begin(), retV.end(), 0.);
      avgRetV /= _problem->_agentsPerEnvironment;
      retV = std::vector<float>(_problem->_agentsPerEnvironment, avgRetV);
    }
  }

  // Now going backwards, setting the retrace value of every experience
  for (ssize_t expId = endId; expId >= startId; expId--)
  {
    for (size_t d = 0; d < _problem->_agentsPerEnvironment; d++)
    {
      // Calculating retrace value. Importance weight is 1.0f because the policy is current.
      retV[d] = getScaledReward(_rewardVector[expId][d], d) + _discountFactor * retV[d];

      // Update value in replay memory
      _retraceValueVector[expId][d] = retV[d];
    }
  }

  // Update reward rescaling sigma
  if (_rewardRescalingEnabled)
    for (size_t d = 0; d < _problem->_agentsPerEnvironment; d++)
      _rewardRescalingSigma[d] = std::sqrt(_rewardRescalingSumSquaredRewards[d] / (float)_rewardVector.size() + 1e-9);

  // Increasing episode counters
  _sessionEpisodeCount++;
  _currentEpisode++;

  // Increasing total experience counters
  _experienceCount += curExperienceCount;
  _sessionExperienceCount += curExperienceCount;
}

std::vector<std::pair<size_t,size_t>> __className__::generateMiniBatch()
{
  // Allocating storage for mini batch experiecne indexes
  std::vector<std::pair<size_t,size_t>> miniBatch( _miniBatchSize * _problem->_agentsPerEnvironment );

  float x;
  size_t expId, agentId;  
  for ( size_t b = 0; b < _miniBatchSize; b++ )
  {
    if( _multiAgentSampling == "Tuples" )
    {
      // Producing random (uniform) number for the selection of the experience
      x = _uniformGenerator->getRandomNumber();

      // Selecting experience
      expId = std::floor(x * (float)(_stateVector.size() - 1));
    }

    // Get agentId and set expId
    for( size_t a = 0; a < _problem->_agentsPerEnvironment; a++ )
    {
      if( (_multiAgentSampling == "Experiences") || (_multiAgentSampling == "Agents") )
      {
        // Producing random (uniform) number for the selection of the experience
        x = _uniformGenerator->getRandomNumber();

        // Selecting experience
        expId = std::floor(x * (float)(_stateVector.size() - 1));
      }

      if( (_multiAgentSampling == "Tuples") || (_multiAgentSampling == "Agents") )
      {
        agentId = a;
      }

      if( _multiAgentSampling == "Experiences" )
      {
        // Producing random (uniform) number for the selection of the agent
        x = _uniformGenerator->getRandomNumber();

        // Selecting agent
        agentId = std::floor(x * (float)(_problem->_agentsPerEnvironment - 1));
      }

      miniBatch[ a * _miniBatchSize + b ].first  = expId;
      miniBatch[ a * _miniBatchSize + b ].second = agentId;
    }
  }

  // Returning generated minibatch
  return miniBatch;
}

std::vector<std::vector<std::vector<float>>> __className__::getMiniBatchStateSequence(const std::vector<std::pair<size_t,size_t>> &miniBatch)
{
  // Getting mini batch size
  const size_t miniBatchSize = miniBatch.size();

  // Allocating state sequence vector
  std::vector<std::vector<std::vector<float>>> stateSequence;
  stateSequence.resize( miniBatchSize );

  #pragma omp parallel for
  for (size_t b = 0; b < miniBatchSize; b++)
  {
    // Getting current expId
    const size_t expId = miniBatch[b].first;

    // Getting current expId
    const size_t agentId = miniBatch[b].second;

    // Getting starting expId
    const size_t startId = getTimeSequenceStartExpId(expId);

    // Calculating time sequence length
    const size_t T = expId - startId + 1;

    // Resize stateSequence
    stateSequence[b].resize(T);
    for(size_t t = 0; t < T; t++)
      stateSequence[b][t].resize(_problem->_stateVectorSize);

    // Now adding states (and actions, if required)
    for (size_t t = 0; t < T; t++)
    {
      size_t curId = startId + t;

      // Write resulting vector to stateSequence
      stateSequence[b][t] = _stateVector[curId][agentId];
    }
  }
  return stateSequence;
}

void __className__::updateExperienceMetadata( const std::vector<std::pair<size_t,size_t>> &miniBatch, const std::vector<policy_t> &policyData, const size_t policyIdx)
{
  /* Sorting minibatch while keeping track of indices
   * helps to quickly detect duplicates when updating metadata
   * makes sure policyData is correctly adressed
   */

  // Create vector of pairs with index and expId
  std::vector<std::pair<size_t,size_t>> sortHelper(miniBatch.size());
  for( size_t b = 0; b<miniBatch.size(); b++ )
  {
    sortHelper[b].first = miniBatch[b].first;
    sortHelper[b].second = b;
  }

  // Stable sort according to expId (avoids reordering of equal first index)
  std::stable_sort(sortHelper.begin(), sortHelper.end());

  // Container for unique experiences
  std::vector<std::pair<size_t,std::vector<size_t>>> updateBatch;

  // Variables to keep track of expId
  size_t expId, curExpId;
  expId = curExpId = miniBatch[sortHelper[0].second].first;

  // Container for agentIds
  std::vector<size_t> agentIds{miniBatch[sortHelper[0].second].second};

  // Container for reference to policyData
  std::vector<size_t> policyIndices{sortHelper[0].second};

  // Creating a selection of unique experiences from the mini batch
  for( size_t b = 1; b < miniBatch.size(); b++ )
  {
    // Get index that sorts this element
    size_t sortingIndex = sortHelper[b].second;

    // Skip if we have two times the same tuple in the minibatch
    if( miniBatch[sortingIndex] == miniBatch[sortHelper[b - 1].second] )
      continue;

    // Append Index
    policyIndices.push_back(sortingIndex);

    // Read next expId
    curExpId = miniBatch[sortingIndex].first;

    if( curExpId != expId )
    {
      // New experience, add add expId and agentIds to updateBatch
      updateBatch.push_back(std::make_pair(expId, agentIds));

      // Update expId and clear agentId-vector
      expId = curExpId;
      agentIds.clear();
    }

    // Add agentId to container for current expId
    agentIds.push_back(miniBatch[sortingIndex].second);
  }
  // Append last entry
  updateBatch.push_back(std::make_pair(expId, agentIds));

  // Run update on sorted minibatch
  size_t updateBatchSize = updateBatch.size();
  std::vector<int> deltaOffPolicyCount(_problem->_agentsPerEnvironment, 0);
  #pragma omp parallel for reduction(vec_int_plus : deltaOffPolicyCount)
  for (size_t b = 0; b < updateBatchSize; b++)
  {
    const size_t expId = updateBatch[b].first;
    const std::vector<size_t> agentIds = updateBatch[b].second;

    // Get reference to data in replay memory
    auto &stateValues = _stateValueVector[expId];
    auto &importanceWeights = _importanceWeightVector[expId];
    auto &truncatedImportanceWeights = _truncatedImportanceWeightVector[expId];
    auto &isOnPolicy = _isOnPolicyVector[expId];

    // Get action and policy for this experience
    const auto &expAction = _actionVector[expId];
    const auto &expPolicy = _expPolicyVector[expId];

    for ( const auto& agentId : agentIds )
    {
      const auto &curPolicy = policyData[policyIndices[b]];

      /* Write data to replay memory */

      // Update state value
      stateValues[agentId] = curPolicy.stateValue;
      if (std::isfinite(stateValues[agentId]) == false)
        KORALI_LOG_ERROR("Calculated state value returned an invalid value: %f\n", stateValues[agentId]);

      // For terminal state update truncated state value
      if (_terminationVector[expId] == e_truncated)
      {
        // Get truncated state
        auto expTruncatedStateSequence = getTruncatedStateSequence(expId, agentId);

        // Forward policy for truncated state
        std::vector<policy_t> policyInfo;
        runPolicy( {expTruncatedStateSequence}, policyInfo, policyIdx );

        // Update truncated state value
        auto &truncatedStateValue = _truncatedStateValueVector[expId];
        if ( truncatedStateValue.size() == 0 )
          truncatedStateValue.resize(_problem->_agentsPerEnvironment);
        truncatedStateValue[agentId] = policyInfo[0].stateValue;

        // Check value of trucated value
        if (std::isfinite(truncatedStateValue[agentId]) == false)
          KORALI_LOG_ERROR("Calculated state value for truncated state returned an invalid value: %f\n", truncatedStateValue[agentId]);

        // For cooporative multi-agent model truncated state-values are averaged
        if (_multiAgentRelationship == "Cooperation")
        {
          // .. therefore also update other truncated state values
          for( size_t a = 0; a < _problem->_agentsPerEnvironment; a++ ) 
          {
            if( a == agentId ) continue;

            // Get truncated state
            auto expTruncatedStateSequence = getTruncatedStateSequence(expId, a);

            // Forward Policy for truncated state
            std::vector<policy_t> policyInfo;
            runPolicy( {expTruncatedStateSequence}, policyInfo, policyIdx );

            // Update Truncated State Value
            truncatedStateValue[a] = policyInfo[0].stateValue;

            // Check value of trucated value
            if (std::isfinite(truncatedStateValue[a]) == false)
              KORALI_LOG_ERROR("Calculated state value for truncated state returned an invalid value: %f\n", truncatedStateValue[a]);
          }

          float avgTruncV = std::accumulate(truncatedStateValue.begin(), truncatedStateValue.end(), 0.);
          avgTruncV /= _problem->_agentsPerEnvironment;
          truncatedStateValue = std::vector<float>(_problem->_agentsPerEnvironment, avgTruncV);
        }
      }

      // Update importance weight
      importanceWeights[agentId] = calculateImportanceWeight(expAction[agentId], curPolicy, expPolicy[agentId]);
      if (std::isfinite(importanceWeights[agentId]) == false)
        KORALI_LOG_ERROR("Calculated value of importanceWeight returned an invalid value: %f\n", importanceWeights[agentId]);

      truncatedImportanceWeights[agentId] = std::min(_importanceWeightTruncationLevel, importanceWeights[agentId]);

      // Update on-policyness
      if( not _multiAgentCorrelation )
      {
        const bool onPolicy = (importanceWeights[agentId] > (1.0f / _experienceReplayOffPolicyCurrentCutoff)) && (importanceWeights[agentId] < _experienceReplayOffPolicyCurrentCutoff);

        // Updating off policy count if a change is detected
        if ( isOnPolicy[agentId] == true && onPolicy == false)
          deltaOffPolicyCount[agentId] += 1;

        if ( isOnPolicy[agentId] == false && onPolicy == true)
          deltaOffPolicyCount[agentId] += -1;

        // Overwrite old value
        isOnPolicy[agentId] = onPolicy;
      }

      // If all checks succeed and derived quantities are stored, store current policy
      _curPolicyVector[expId][agentId] = curPolicy;
    }

    // After iterating over agents for this experience, update on-policyness for
    if( _multiAgentCorrelation )
    {
      float logProdImportanceWeight = 0.0;
      for( size_t a = 0; a < _problem->_agentsPerEnvironment; a++ )
        logProdImportanceWeight += std::log(importanceWeights[a]);

      const double logCutOff = (double)_problem->_agentsPerEnvironment * std::log(_experienceReplayOffPolicyCurrentCutoff);
      const bool onPolicy = (logProdImportanceWeight > (-1. * logCutOff)) && (logProdImportanceWeight < logCutOff);

      // Updating off policy count if a change is detected
      for( size_t a = 0; a < _problem->_agentsPerEnvironment; a++ )
      if ( isOnPolicy[a] == true && onPolicy == false)
        deltaOffPolicyCount[a]++;

      for( size_t a = 0; a < _problem->_agentsPerEnvironment; a++ )
      if ( isOnPolicy[a] == false && onPolicy == true)
        deltaOffPolicyCount[a]--;

      // Overwrite old value
      std::fill(isOnPolicy.begin(), isOnPolicy.end(), onPolicy);
    }
  }

  // Update Off-Policy count and ratio
  for( size_t a = 0; a<_problem->_agentsPerEnvironment; a++ )
  {
    _experienceReplayOffPolicyCount[a] += deltaOffPolicyCount[a];
    _experienceReplayOffPolicyRatio[a] = (float)_experienceReplayOffPolicyCount[a] / (float)(_stateVector.size());
  }

  // Now filtering experiences from the same episode
  std::vector<std::pair<size_t,std::vector<size_t>>> retraceMiniBatch;

  // Adding last experience from the sorted minibatch
  retraceMiniBatch.push_back(updateBatch[updateBatchSize - 1]);

  // Adding experiences so long as they do not repeat episodes
  for (ssize_t b = updateBatchSize - 2; b >= 0; b--)
  {
    size_t currExpId = updateBatch[b].first;
    size_t nextExpId = updateBatch[b + 1].first;
    size_t curEpisode = _episodeIdVector[currExpId];
    size_t nextEpisode = _episodeIdVector[nextExpId];
    if (curEpisode != nextEpisode) retraceMiniBatch.push_back(updateBatch[b]);
  }

  // Calculating retrace value starting from oldest experiences
  #pragma omp parallel for
  for (size_t b = 0; b < retraceMiniBatch.size(); b++ )
  {
    // Finding the earliest experience corresponding to the same episode as this experience
    ssize_t endId = retraceMiniBatch[b].first;
    ssize_t startId = endId - _episodePosVector[endId];

    // If the starting experience has already been discarded, take the earliest one that still remains
    if (startId < 0) startId = 0;

    // Get agentIds that correspond to this experience
    const std::vector<size_t> agentIds = retraceMiniBatch[b].second;

    // Storage for the retrace value
    auto &retV = _retraceValueVector[endId];

    // If it was a truncated episode, add the value function for the terminal state to retV
    if (_terminationVector[endId] == e_truncated)
    for ( const auto& agentId : agentIds )
      retV[agentId] = _truncatedStateValueVector[endId][agentId];

    if (_terminationVector[endId] == e_nonTerminal)
    for ( const auto& agentId : agentIds )
      retV[agentId] = _retraceValueVector[endId + 1][agentId];

    // Now iterating backwards to calculate the rest of vTbc
    for (ssize_t curId = endId; curId >= startId; curId--)
    {
      std::vector<float> truncatedImportanceWeights = _truncatedImportanceWeightVector[curId];

      // Calculate truncated product of importance weights for multi-agent correlation
      if ( _multiAgentCorrelation ) 
      {
        float logProdImportanceWeight = 0.0f;
        for (size_t d = 0; d < _problem->_agentsPerEnvironment; d++)
          logProdImportanceWeight += std::log(_importanceWeightVector[curId][d]);
        
        float truncatedProdImportanceWeight = std::min(_importanceWeightTruncationLevel, std::exp(logProdImportanceWeight));
  
        truncatedImportanceWeights = std::vector<float>(_problem->_agentsPerEnvironment, truncatedProdImportanceWeight);
      }

      // Get state-value and replace by avg for cooporating multi-agent setting
      std::vector<float> curV = _stateValueVector[curId];
      if (_multiAgentRelationship == "Cooperation")
      {
        float avgV = std::accumulate(curV.begin(), curV.end(), 0.);
        avgV /= _problem->_agentsPerEnvironment;
        curV = std::vector<float>(_problem->_agentsPerEnvironment, avgV);
      }

      // Update retrace values for all agents for this experience
      for ( const auto& agentId : agentIds )
      {
        // Get reward
        auto curReward = getScaledReward(_rewardVector[curId][agentId], agentId);

        // Apply recursion
        retV[agentId] = curV[agentId] + truncatedImportanceWeights[agentId] * (curReward + _discountFactor * retV[agentId] - curV[agentId]);
      }
    }
  }
}

size_t __className__::getTimeSequenceStartExpId(size_t expId)
{
  size_t startId = expId;

  // Adding (tmax-1) time sequences to the given experience
  for (size_t t = 0; t < _timeSequenceLength - 1; t++)
  {
    // If we reached the start of the ER, this is the starting episode in the sequence
    if (startId == 0) break;

    // Now going back one experience
    startId--;

    // If we reached the end of the previous episode, then add one (this covers the case where the provided experience is also terminal) and break.
    if (_terminationVector[startId] != e_nonTerminal)
    {
      startId++;
      break;
    }
  }

  return startId;
}

void __className__::resetTimeSequence()
{
  _stateTimeSequence.clear();
}

std::vector<std::vector<float>> __className__::getTruncatedStateSequence(size_t expId, size_t agentId)
{
  // Getting starting expId
  size_t startId = getTimeSequenceStartExpId(expId);

  // Creating storage for the time sequence
  std::vector<std::vector<float>> timeSequence;

  // Now adding states, except for the initial one
  for (size_t e = startId + 1; e <= expId; e++)
    timeSequence.push_back(_stateVector[e][agentId]);

  // Lastly, adding truncated state
  timeSequence.push_back(_truncatedStateVector[expId][agentId]);

  return timeSequence;
}

void __className__::finalize()
{
  if (_mode != "Training") return;

  if (_experienceReplaySerialize == true)
    if (_k->_fileOutputEnabled)
      serializeExperienceReplay();

  _k->_logger->logInfo("Normal", "Waiting for pending agents to finish...\n");

  // Waiting for pending agents to finish
  bool agentsRemain = true;
  do
  {
    agentsRemain = false;
    for (size_t agentId = 0; agentId < _concurrentEnvironments; agentId++)
      if (_isAgentRunning[agentId] == true)
      {
        attendAgent(agentId);
        agentsRemain = true;
      }

    if (agentsRemain) KORALI_LISTEN(_agents);
  } while (agentsRemain == true);
}

void __className__::serializeExperienceReplay()
{
  _k->_logger->logInfo("Detailed", "Serializing Agent's Training State...\n");
  auto beginTime = std::chrono::steady_clock::now(); // Profiling

  // Creating JSON storage variable
  knlohmann::json stateJson;

  // Serializing agent's database into the JSON storage
  for (size_t i = 0; i < _stateVector.size(); i++)
  {
    stateJson["Experience Replay"][i]["Episode Id"] = _episodeIdVector[i];
    stateJson["Experience Replay"][i]["Episode Pos"] = _episodePosVector[i];
    stateJson["Experience Replay"][i]["State"] = _stateVector[i];
    stateJson["Experience Replay"][i]["Action"] = _actionVector[i];
    stateJson["Experience Replay"][i]["Reward"] = _rewardVector[i];
    stateJson["Experience Replay"][i]["State Value"] = _stateValueVector[i];
    stateJson["Experience Replay"][i]["Retrace Value"] = _retraceValueVector[i];
    stateJson["Experience Replay"][i]["Importance Weight"] = _importanceWeightVector[i];
    stateJson["Experience Replay"][i]["Truncated Importance Weight"] = _truncatedImportanceWeightVector[i];
    stateJson["Experience Replay"][i]["Is On Policy"] = _isOnPolicyVector[i];
    stateJson["Experience Replay"][i]["Truncated State"] = _truncatedStateVector[i];
    stateJson["Experience Replay"][i]["Truncated State Value"] = _truncatedStateValueVector[i];
    stateJson["Experience Replay"][i]["Termination"] = _terminationVector[i];

    std::vector<float> expStateValue(_problem->_agentsPerEnvironment, 0.0f);
    std::vector<std::vector<float>> expDistributionParameter(_problem->_agentsPerEnvironment, std::vector<float>(_expPolicyVector[0][0].distributionParameters.size()));
    std::vector<size_t> expActionIdx(_problem->_agentsPerEnvironment, 0);
    std::vector<std::vector<float>> expUnboundedAct(_problem->_agentsPerEnvironment, std::vector<float>(_expPolicyVector[0][0].unboundedAction.size()));
    std::vector<std::vector<float>> expActProb(_problem->_agentsPerEnvironment, std::vector<float>(_expPolicyVector[0][0].actionProbabilities.size()));

    std::vector<float> curStateValue(_problem->_agentsPerEnvironment, 0.0f);
    std::vector<std::vector<float>> curDistributionParameter(_problem->_agentsPerEnvironment, std::vector<float>(_curPolicyVector[0][0].distributionParameters.size()));
    std::vector<size_t> curActionIdx(_problem->_agentsPerEnvironment, 0);
    std::vector<std::vector<float>> curUnboundedAct(_problem->_agentsPerEnvironment, std::vector<float>(_curPolicyVector[0][0].unboundedAction.size()));
    std::vector<std::vector<float>> curActProb(_problem->_agentsPerEnvironment, std::vector<float>(_curPolicyVector[0][0].actionProbabilities.size()));
    std::vector<std::vector<bool>> curAvailAct(_problem->_agentsPerEnvironment, std::vector<bool>(_curPolicyVector[0][0].availableActions.size()));

    for (size_t j = 0; j < _problem->_agentsPerEnvironment; j++)
    {
      expStateValue[j] = _expPolicyVector[i][j].stateValue;
      expDistributionParameter[j] = _expPolicyVector[i][j].distributionParameters;
      expActionIdx[j] = _expPolicyVector[i][j].actionIndex;
      expUnboundedAct[j] = _expPolicyVector[i][j].unboundedAction;
      expActProb[j] = _expPolicyVector[i][j].actionProbabilities;

      curStateValue[j] = _curPolicyVector[i][j].stateValue;
      curDistributionParameter[j] = _curPolicyVector[i][j].distributionParameters;
      curActionIdx[j] = _curPolicyVector[i][j].actionIndex;
      curUnboundedAct[j] = _curPolicyVector[i][j].unboundedAction;
      curActProb[j] = _curPolicyVector[i][j].actionProbabilities;
      curAvailAct[j] = _curPolicyVector[i][j].availableActions;
    }
    stateJson["Experience Replay"][i]["Experience Policy"]["State Value"] = expStateValue;
    stateJson["Experience Replay"][i]["Experience Policy"]["Distribution Parameters"] = expDistributionParameter;
    stateJson["Experience Replay"][i]["Experience Policy"]["Action Index"] = expActionIdx;
    stateJson["Experience Replay"][i]["Experience Policy"]["Unbounded Action"] = expUnboundedAct;
    stateJson["Experience Replay"][i]["Experience Policy"]["Action Probabilities"] = expActProb;

    stateJson["Experience Replay"][i]["Current Policy"]["State Value"] = curStateValue;
    stateJson["Experience Replay"][i]["Current Policy"]["Distribution Parameters"] = curDistributionParameter;
    stateJson["Experience Replay"][i]["Current Policy"]["Action Index"] = curActionIdx;
    stateJson["Experience Replay"][i]["Current Policy"]["Unbounded Action"] = curUnboundedAct;
    stateJson["Experience Replay"][i]["Current Policy"]["Action Probabilities"] = curActProb;
    stateJson["Experience Replay"][i]["Current Policy"]["Available Actions"] = curAvailAct;
  }

  // If results directory doesn't exist, create it
  if (!dirExists(_k->_fileOutputPath)) mkdir(_k->_fileOutputPath);

  // Resolving file path
  std::string statePath = _k->_fileOutputPath + "/state.json";

  // Storing database to file
  if (saveJsonToFile(statePath.c_str(), stateJson) != 0)
    KORALI_LOG_ERROR("Could not serialize training state into file %s\n", statePath.c_str());

  auto endTime = std::chrono::steady_clock::now();                                                                   // Profiling
  _sessionSerializationTime += std::chrono::duration_cast<std::chrono::nanoseconds>(endTime - beginTime).count();    // Profiling
  _generationSerializationTime += std::chrono::duration_cast<std::chrono::nanoseconds>(endTime - beginTime).count(); // Profiling
}

void __className__::deserializeExperienceReplay()
{
  auto beginTime = std::chrono::steady_clock::now(); // Profiling

  // Creating JSON storage variable
  knlohmann::json stateJson;

  // Resolving file path
  std::string statePath = _k->_fileOutputPath + "/state.json";

  // Loading database from file
  _k->_logger->logInfo("Normal", "Loading previous run training state from file %s...\n", statePath.c_str());
  if (loadJsonFromFile(stateJson, statePath.c_str()) == false)
    KORALI_LOG_ERROR("Trying to resume training or test policy but could not find or deserialize agent's state from from file %s...\n", statePath.c_str());

  // Clearing existing database
  _stateVector.clear();
  _actionVector.clear();
  _retraceValueVector.clear();
  _rewardVector.clear();
  _stateValueVector.clear();
  _importanceWeightVector.clear();
  _truncatedImportanceWeightVector.clear();
  _truncatedStateValueVector.clear();
  _truncatedStateVector.clear();
  _terminationVector.clear();
  _expPolicyVector.clear();
  _curPolicyVector.clear();
  _isOnPolicyVector.clear();
  _priorityVector.clear();
  _probabilityVector.clear();
  _episodePosVector.clear();
  _episodeIdVector.clear();

  // Deserializing database from JSON to the agent's state
  for (size_t i = 0; i < stateJson["Experience Replay"].size(); i++)
  {
    _episodeIdVector.add(stateJson["Experience Replay"][i]["Episode Id"].get<size_t>());
    _episodePosVector.add(stateJson["Experience Replay"][i]["Episode Pos"].get<size_t>());
    _stateVector.add(stateJson["Experience Replay"][i]["State"].get<std::vector<std::vector<float>>>());
    _actionVector.add(stateJson["Experience Replay"][i]["Action"].get<std::vector<std::vector<float>>>());
    _rewardVector.add(stateJson["Experience Replay"][i]["Reward"].get<std::vector<float>>());
    _stateValueVector.add(stateJson["Experience Replay"][i]["State Value"].get<std::vector<float>>());
    _retraceValueVector.add(stateJson["Experience Replay"][i]["Retrace Value"].get<std::vector<float>>());
    _importanceWeightVector.add(stateJson["Experience Replay"][i]["Importance Weight"].get<std::vector<float>>());
    _truncatedImportanceWeightVector.add(stateJson["Experience Replay"][i]["Truncated Importance Weight"].get<std::vector<float>>());
    _isOnPolicyVector.add(stateJson["Experience Replay"][i]["Is On Policy"].get<std::vector<bool>>());
    _truncatedStateVector.add(stateJson["Experience Replay"][i]["Truncated State"].get<std::vector<std::vector<float>>>());
    _truncatedStateValueVector.add(stateJson["Experience Replay"][i]["Truncated State Value"].get<std::vector<float>>());
    _terminationVector.add(stateJson["Experience Replay"][i]["Termination"].get<termination_t>());

    std::vector<policy_t> expPolicy(_problem->_agentsPerEnvironment);
    std::vector<policy_t> curPolicy(_problem->_agentsPerEnvironment);
    for (size_t d = 0; d < _problem->_agentsPerEnvironment; d++)
    {
      expPolicy[d].stateValue = stateJson["Experience Replay"][i]["Experience Policy"]["State Value"][d].get<float>();
      expPolicy[d].distributionParameters = stateJson["Experience Replay"][i]["Experience Policy"]["Distribution Parameters"][d].get<std::vector<float>>();
      expPolicy[d].unboundedAction = stateJson["Experience Replay"][i]["Experience Policy"]["Unbounded Action"][d].get<std::vector<float>>();
      expPolicy[d].actionIndex = stateJson["Experience Replay"][i]["Experience Policy"]["Action Index"][d].get<size_t>();
      expPolicy[d].actionProbabilities = stateJson["Experience Replay"][i]["Experience Policy"]["Action Probabilities"][d].get<std::vector<float>>();

      curPolicy[d].stateValue = stateJson["Experience Replay"][i]["Current Policy"]["State Value"][d].get<float>();
      curPolicy[d].distributionParameters = stateJson["Experience Replay"][i]["Current Policy"]["Distribution Parameters"][d].get<std::vector<float>>();
      curPolicy[d].actionIndex = stateJson["Experience Replay"][i]["Current Policy"]["Action Index"][d].get<size_t>();
      curPolicy[d].unboundedAction = stateJson["Experience Replay"][i]["Current Policy"]["Unbounded Action"][d].get<std::vector<float>>();
      curPolicy[d].actionProbabilities = stateJson["Experience Replay"][i]["Current Policy"]["Action Probabilities"][d].get<std::vector<float>>();
      curPolicy[d].availableActions = stateJson["Experience Replay"][i]["Current Policy"]["Available Actions"][d].get<std::vector<bool>>();
    }
    _expPolicyVector.add(expPolicy);
    _curPolicyVector.add(curPolicy);
  }

  auto endTime = std::chrono::steady_clock::now();                                                                         // Profiling
  double deserializationTime = std::chrono::duration_cast<std::chrono::nanoseconds>(endTime - beginTime).count() / 1.0e+9; // Profiling
  _k->_logger->logInfo("Normal", "Took %fs to deserialize training state.\n", deserializationTime);
}

void __className__::printGenerationAfter()
{
  if (_mode == "Training")
  {
    _k->_logger->logInfo("Normal", "Experience Replay Statistics:\n");

    _k->_logger->logInfo("Normal", " + Experience Memory Size:      %lu/%lu\n", _stateVector.size(), _experienceReplayMaximumSize);
    if (_maxEpisodes > 0)
      _k->_logger->logInfo("Normal", " + Total Episodes Count:        %lu/%lu\n", _currentEpisode, _maxEpisodes);
    else
      _k->_logger->logInfo("Normal", " + Total Episodes Count:        %lu\n", _currentEpisode);

    if (_maxExperiences > 0)
      _k->_logger->logInfo("Normal", " + Total Experience Count:      %lu/%lu\n", _experienceCount, _maxExperiences);
    else
      _k->_logger->logInfo("Normal", " + Total Experience Count:      %lu\n", _experienceCount);

    _k->_logger->logInfo("Normal", "Training Statistics:\n");
    if (_maxPolicyUpdates > 0)
      _k->_logger->logInfo("Normal", " + Policy Update Count:         %lu/%lu\n", _policyUpdateCount, _maxPolicyUpdates);
    else
      _k->_logger->logInfo("Normal", " + Policy Update Count:         %lu\n", _policyUpdateCount);

    for (size_t d = 0; d < _problem->_agentsPerEnvironment; d++)
    {
      _k->_logger->logInfo("Normal", "Off-Policy Statistics for agent %lu: \n", d);
      _k->_logger->logInfo("Normal", " + Count (Ratio/Target):        %lu/%lu (%.3f/%.3f)\n", _experienceReplayOffPolicyCount[d], _stateVector.size(), _experienceReplayOffPolicyRatio[d], _experienceReplayOffPolicyTarget);
      _k->_logger->logInfo("Normal", " + Importance Weight Cutoff:    [%.3f, %.3f]\n", 1.0f / _experienceReplayOffPolicyCurrentCutoff, _experienceReplayOffPolicyCurrentCutoff);
      _k->_logger->logInfo("Normal", " + REFER Beta Factor:           %f\n", _experienceReplayOffPolicyREFERCurrentBeta[d]);

      _k->_logger->logInfo("Normal", " + Latest Reward for agent %lu:               %f\n", d, _trainingLastReward[d]);
      _k->_logger->logInfo("Normal", " + %lu-Episode Average Reward for agent %lu:  %f\n", _trainingAverageDepth, d, _trainingAverageReward[d]);
      _k->_logger->logInfo("Normal", " + Best Reward for agent %lu:                 %f (%lu)\n", d, _trainingBestReward[d], _trainingBestEpisodeId[d]);

      if (_rewardRescalingEnabled)
        _k->_logger->logInfo("Normal", " + Reward Rescaling:            N(%.3e, %.3e)         \n", 0.0, _rewardRescalingSigma[d]);
    }

    if (_testingBestEpisodeId > 0)
    {
      _k->_logger->logInfo("Normal", "Testing Statistics:\n");

      _k->_logger->logInfo("Normal", " + Candidate Policies:          %lu\n", _testingCandidateCount);

      _k->_logger->logInfo("Normal", " + Latest Average (Worst / Best) Reward for agent: %f (%f / %f)\n", _testingAverageReward, _testingWorstReward, _testingBestReward);
      _k->_logger->logInfo("Normal", " + Best Average Reward for agent: %f (%lu)\n", _testingBestAverageReward, _testingBestEpisodeId);
    }

    printAgentInformation();
    _k->_logger->logInfo("Normal", " + Current Learning Rate:           %.3e\n", _currentLearningRate);

    if (_stateRescalingEnabled)
      _k->_logger->logInfo("Normal", " + Using State Rescaling\n");

    _k->_logger->logInfo("Detailed", "Profiling Information:                  [Generation] - [Session]\n");
    _k->_logger->logInfo("Detailed", " + Experience Serialization Time:       [%5.3fs] - [%3.3fs]\n", _generationSerializationTime / 1.0e+9, _sessionSerializationTime / 1.0e+9);
    _k->_logger->logInfo("Detailed", " + Agent Attending Time:                [%5.3fs] - [%3.3fs]\n", _generationAgentAttendingTime / 1.0e+9, _sessionAgentAttendingTime / 1.0e+9);
    _k->_logger->logInfo("Detailed", " + Avg Agent Computation Time:          [%5.3fs] - [%3.3fs]\n", _generationAgentComputationTime / 1.0e+9, _sessionAgentComputationTime / 1.0e+9);
    _k->_logger->logInfo("Detailed", " + Avg Agent Communication/Wait Time:   [%5.3fs] - [%3.3fs]\n", _generationAgentCommunicationTime / 1.0e+9, _sessionAgentCommunicationTime / 1.0e+9);
    _k->_logger->logInfo("Detailed", " + Avg Agent Policy Evaluation Time:    [%5.3fs] - [%3.3fs]\n", _generationAgentPolicyEvaluationTime / 1.0e+9, _sessionAgentPolicyEvaluationTime / 1.0e+9);
    _k->_logger->logInfo("Detailed", " + Policy Update Time:                  [%5.3fs] - [%3.3fs]\n", _generationPolicyUpdateTime / 1.0e+9, _sessionPolicyUpdateTime / 1.0e+9);
    _k->_logger->logInfo("Detailed", "   + generateMiniBatch Time:            [%5.3fs] \n", _generationMiniBatchGenerationTime / 1.0e+9);
    _k->_logger->logInfo("Detailed", "   + getMiniBatchStateSequence Time:    [%5.3fs] \n", _generationStateSequenceTime / 1.0e+9);
    _k->_logger->logInfo("Detailed", "   + runPolicy Time:                    [%5.3fs] \n", _generationRunPolicyTime / 1.0e+9);
    _k->_logger->logInfo("Detailed", "   + updateExperienceMetadeta Time:     [%5.3fs] \n", _generationUpdateExperienceMetadataTime/ 1.0e+9);
    _k->_logger->logInfo("Detailed", "   + calculatePolicyGradients Time:     [%5.3fs] \n", _generationCalculatePolicyGradientsTime / 1.0e+9);
    _k->_logger->logInfo("Detailed", "   + runGeneration Time                 [%5.3fs] \n", _generationRunGenerationTime/ 1.0e+9);
    _k->_logger->logInfo("Detailed", " + Running Time:                        [%5.3fs] - [%3.3fs]\n", _generationRunningTime / 1.0e+9, _sessionRunningTime / 1.0e+9);
    _k->_logger->logInfo("Detailed", " + [I/O] Result File Saving Time:        %5.3fs\n", _k->_resultSavingTime / 1.0e+9);
  }

  if (_mode == "Testing")
  {
    _k->_logger->logInfo("Normal", "Testing Results:\n");
    for (size_t sampleId = 0; sampleId < _testingSampleIds.size(); sampleId++)
    {
      _k->_logger->logInfo("Normal", " + Sample %lu:\n", _testingSampleIds[sampleId]);
      _k->_logger->logInfo("Normal", "   + (Average) Cumulative Reward:           %f\n", _testingReward[sampleId]);
    }
  }
}

__moduleAutoCode__;

__endNamespace__;
